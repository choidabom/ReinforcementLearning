# 1장 강화학습 개요
: 강화학습의 개념과 강화학습 문제의 정의

알파고와 이세돌의 세기의 대결 이후 가장 주목 받았던 분야는 바로 **딥러닝**이다.

들어보긴 많이 들어본 딥러닝... 영어로 하면 **Deep Neural Network이라 DNN**이라고 부르기도 한다.

딥러닝은 사람의 뇌를 간단하게 모방한 "인공신경망"을 이용한 학습 방법이다~! 

딥러닝은 주로 시각에 관련된 데이터를 처리하는 데 탁월한 성능을 자랑한다고 한다. (시각에 관련된 데이터?!!?)


이세돌 9단을 이겼던 알파고는 구글 딥마인드의 컴퓨터 바독 인공지능 프로그램이다.

**사실 알파고는 딥러닝만으로 만들어진 것이 아니다..!**

알파고는 바둑의 규칙을 모르지만 직접 바둑을 두면서 어떻게 해야 상대방을 이길 수 있는지 학습을 하는 과정을 거쳤고 규칙을 모르지만 학습할 수 있었던 것은 강화학습을 사용했기 때문이라고 한다..!!! 

규칙을 모르는데 학습을 어떻게 하는겨?!! 이게 이제 강화학습의 시작점이라고 할 수 있다.
* * *

### 강화학습의 공부 순서
강화학습은 **행동심리학**과 **머신러닝**에 뿌리를 두었기 때문에 두 가지 관점에서 살펴보아야한다.

강화학습은 다른 머신러닝과 다르게 순차적으로 행동을 결정해야 하는 문제를 다룬다. 이를 풀기 위해 문제를 수학적으로 잘 정의해야한다. (수학적으로 어떻게 정의할 수 있는가...?)

* * *

## 강화학습의 개념
### 스키너의 강화 연구

강화학습의 개념
강화 = "시행착오"를 통해 학습하는 방법 중 하나

강화라는 개념을 첨으로 제시한 사람이 바로 스키너이다.
```
[스키너의 쥐 실험]
1. 굶긴 쥐를 상자에 넣는다.
2. 쥐는 돌아다니다가 우연히 상자 안에 있는 지렛대를 누르게 된다.
3. 지렛대를 누르자 먹이가 나온다.
4.지렛대를 누르는 행동과 먹이와의 상관관계를 모르는 쥐는 다시 돌아다닌다.
5. 그러다가 우연히 쥐가 다시 지렛대를 누르면 쥐는 이제 먹이와 지렛대 사이의 관계를 알게 되고 점점 지렛대를 자주 누르게 된다.
6. 이 과정을 반복하면서 쥐는 지렛대를 누르면 먹이를 먹을 수 있다는 것을 학습한다.
```
즉, 강화는 이전에 배우지 않았지만 직접 시도하면서 행동과 그 결과로 나타나는 좋은 보상 사이의 상관관계를 학습하는 것이다. 강화의 핵심은 "쥐가 점점 보상을 얻게 해주는 행동을 자주 한다는 것!" 이다.
* * *

### 머신러닝과 강화학습
강화학습을 정의하려면 다른 머신러닝에 대해 알아야한다.

**머신러닝**은 인공지능의 한 범주로, 컴퓨터가 스스로 학습하게 하는 알고리즘을 개발하는 분야이다.

머신러닝은 크게 지도학습, 비지도학습, 강화학습으로 나뉜다.

- **지도학습** = "정답"을 알고 있는 데이터를 이용해 컴퓨터를 학습시킴. 컴퓨터가 낸 답과 정답의 차이를 통해 지속 학습. 회귀분석(Regressin)&분류(Classification)

- **비지도학습** = 정답 없이 주어진 데이터로만 학습. 대표적으로 군집화(Clustering)이 해당.

- **강화학습** = **"보상(Reward)"**를 통해 학습. 보상은 컴퓨터가 선택한 **행동(Action)**에 대한 환경의 반응이다.


강화학습은 정답이 주어진 것도 아니고 그저 주어진 데이터에 대해 학습하는 것도 아님!

=> 강화학습을 수행하는 컴퓨터는 행동심리학에서 살펴본 "강화"처럼 보상을 얻게 하는 행동을 점점 많이 하도록 학습한다!
* * *

### 스스로 학습하는 컴퓨터, 에이전트
* 에이전트: 강화학습을 통해 스스로 학습하는 컴퓨터

에이전트는 환경에 대해 사전지식이 없는 상태에서 학습을 한다.

에이전트는 자신의 행동과 행동의 결과를 보상을 통해 학습하면서 어떤 행동을 해야 좋은 결과를 얻게 되는지 알게 된다.

강화학습의 **목적**은 에이전트가 환경을 탐색하면서 얻는 보상들의 합을 최대화하는 "최적의 행동양식, 또는 정책"을 학습하는 것이다.

보상을 양수와 음수 모두 설정할 수 있고, 적절한 상벌을 통해 무엇이 더 정확한지 알 수 있다. 

강화학습의 **장점**은 환경에 대한 사전지식이 없어도 학습할 수 있다는 것이다.

**다시 돌아와서 알파고를 강화학습 관점에서 본다면 강화학습은 환경에 대한 사전지식이 없어도 학습이 가능하기 때문에 바둑의 규칙을 몰라도 학습할 수 있었던 것이다.!!!**

바둑의 규칙도 몰랐던 알파고(에이전트)는 수많은 모의대국을 치르면서 보상을 받고 상대방을 이기게 한 행동을 더 하면서 세계적인 바둑기사와 바둑을 둘 정도로 성장하게 되었다.
* * *

## 강화학습의 문제

Q. 강화학습은 어떤 문제에 적용할까???

A. 강화학습은 결정을 순차적으로 내려야 하는 문제에 적용된다.

=> **순차적 결정 문제**: 현재 위치에서 행동을 한 번 선택하는 것이 아니라 계속적으로 선택하는 문제 (다이내믹 프로그래밍 or 진화 알고리즘에 적용 가능)

### 순차적 행동 결정 문제
Q. 순차적으로 행동을 결정해야 하는 문제를 어떻게 수학적으로 표현할 수 있을까??? (위에서 말한 것과 같이 수학적으로 잘 정의해야한다.)

A. MDP(Markov Decision Process); 순차적으로 행동을 결정하는 문제를 정의할 때 사용하는 방법이다.
* * *

### 순차적 행동 결정 문제의 구성요소
수학적으로 정의된 문제는 다음과 같은 구성요소를 가진다. 

1. **상태(State)**

: 현재 에이전트의 정보(정적인 요소 + 동적인 요소)

* 상태의 정의 중요, 에이전트가 상태를 통해 상황을 판단해서 행동을 결정하기에 충분한 정보를 제공해야함.
* 예를 들어, 탁구를 치는 에이전트가 탁구공의 위치만 알고 속도를 모른다면 사실상 탁구를 칠 수 없는 것과 마찬가지이다. 탁구 치는 것을 학습하려면 탁구공의 위치, 속도, 가속도 같은 정보가 필요하다.

2. **행동(Action)**

: 에이전트가 어떠한 상태에서 취할 수 있는 행동

* 처음에는 무작위로 행동 -> 에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고 다음 상태를 알려줌.

3. **보상(Reward)**

: 에이전트가 학습할 수 있는 유일한 정보, 자신이 했던 행동을 평가할 수 있는 지표

* 강화학습의 목표는 시간에 따라 얻는 보상의 합을 최대로 하는 정책을 찾는 것
* 보상은 에이전트에 속하지 않는 환경의 일부이다. 에이전트는 어떤 상황에서 얼마의 보상이 나오는지 미리 알지 못 한다.

4. **정책(Policy)**

: 모든 상태에 대해 에이전트가 어떤 행동을 해야하는지 정해놓은 것/**순차적 행동 결정 문제에서 구해야 할 *답***
* So, 순차적 행동 결정 문제를 풀었다 == 제일 좋은 정책을 에이전트가 얻었다 == 최적 정책(optimal policy)
