# 2장 강화학습 기초 1: 
: MDP와 벨만 방정식
* 강화학습이 어떤 방정식을 풀어내는 방법이라면 그 방정식이 무엇인지 아는 것이 중요할 것이다...! 그 방정식은 바로 **벨만 방정식**이다.
* 순차적 행동 결정 문제는 **MDP로 정의 가능**...(1장에서 말했던 것처럼)/순차적 행동 결정 문제의 간단 예시 **그리드월드**를 통해 MDP 알아보기..!
* MDP를 통해 정의된 문제에서 에이전트가 학습하기 위해 **가치함수** 개념 도입...! 근데 가치함수가 벨만 방정식과 연결이 된다..!
* * *

## MDP
: MDP의 구성요소 => 상태, 행동, 보상함수, 상태 변환 확률, 할인율
* MDP의 이해를 돕기 위해 그리드월드(Grid World) 예제 사용
* 그리드 월드: 격자로 이뤄진 환경에서 문제를 푸는 각종 예제
* * *

### 상태
**에이전트가 관찰 가능한 상태의 집합: S**
* (상태라는 말이 모호할 수 있는데) "자신의 상황에 대한 관찰" == 상태
* 그리드 월드에서 상태의 개수는 유한
* 그리드 월드에서 상태는 격자 상의 각 위치(좌표)
* 그리드 월드에 상태가 5개 있을 경우, 수식으로 표현하면 
* ![image](https://user-images.githubusercontent.com/48302257/178220002-fd951711-0b89-4215-abe5-62a8952590a5.png)
* 에이전트는 시간에 따라 상태 집합 안에 있는 상태를 탐험한다. 이 때 시간을 t, 시간 t일 때의 상태를 St라고 표현한다.
* 예를 들어, 시간이 t일 때 상태가 (1, 3)이라면 St=(1, 3)
* 어떤 t에서의 상태 St는 정해진 것이 아니다. 때에 따라 t=1 일 때, St=(1, 3)일 수도 있고 St=(4, 2)일 수도 있다. 

**"상태 = 확률 변수(Random Variable)"**

**St = s => "시간 t에서의 상태 St가 어떤 상태s다."**
* * *

### 행동
**에이전트가 상태 St에서 할 수 있는 가능한 행동의 집합 : A**
* 보통 에이전트가 할 수 있는 행동은 모든 상태에서 같다. At = a
* "시간 t에 에이전트가 특정한 행동 a를 했다."
* t라는 시간에 에이전트가 어떤 행동을 할 지는 정해져 있지 않으므로 At처럼 대문자로 표현한다.
* 그리드 월드에서 에이전트가 할 수 있는 행동은 A = {up, down, left, right}
* 만약 시간 t에서 상태가 (1, 3)이고 At=right 이라면 다음 시간의 상태는 (2, 3)이 된다.
* **만약 예상치 못한 요소가 있다면 에이전트가 특정 행동을 했을 때 어디로 이동할지 결정하는 것이 '상태 변환 확률'이다.**
* * *

### 보상함수
**에이전트가 학습할 수 있는 유일한 정보 == 환경이 에이전트에게 주는 정보**
* 시간 t에서 상태가 St = s 이고, 행동이 At = a 일 때 에이전트가 받는 보상에 대한 기댓값(Expectation) E
* 에이전트가 어떤 상태에서 행동한 시간: t
* 보상을 받는 시간: t+1
* 



### 상태 변환 확률
### 할인율
### 정책

## 가치함수
### 가치함수
### 큐함수

## 벨만 방정식
### 벨만 기대 방정식
### 벨만 최적 방정식
